# Company Hack Day Recap

**Date**: 2025-12-08

## Projects

### 1. Domain Assist for Nx Cloud
**By**: Altan
**Demo**: https://www.loom.com/share/0b8642cfb59b4334b7abca5629fdd4df

**Problem**: The Nx Cloud domain model lives in a 4,000-line Kotlin file. For team members unfamiliar with Kotlin or the historical naming conventions, it's difficult to:
- Find the right model/collection
- Associate field names correctly
- Write queries for ad-hoc reports, customer requests, or dashboards

**Solution**: A tool with two main features:

1. **Interactive Entity Graph** - Visual web/graph of all Nx Cloud entities
   - Powered by annotations on Kotlin models defining relationships
   - Shows indexes, field types, and semantic definitions (generated by Claude)
   - Allows filtering and exploring entity relationships
   - Generated at build time, so always up-to-date with any commit

2. **Chat Interface for Query Generation** - Ask natural language questions like:
   > "I have a distributed execution with run group abc123, and I want to find all the conformance rule results. I'm just interested in the IDs."

   The system:
   - Analyzes query semantics to find start/end entities
   - Constructs up to 10 traversal paths between entities
   - Uses LLM to select the most semantically accurate path
   - Generates a working MongoDB aggregation pipeline with proper lookups

**Smart Features**:
- Handles field name variations (e.g., `organizationName` vs `organizationId` on different entities)
- Compile-time validation prevents nonsense links or references to non-existent fields
- Index awareness - could prefer indexed paths for better query performance
- Avoids pitfalls like full collection scans

**Value**: Helps build mental model of how entities relate, and makes querying data accessible to anyone regardless of Kotlin/Ocean familiarity.

### 2. OTEL for Nx CLI
**By**: Max and Colum
**Demo**: https://www.loom.com/share/d8a3c759c6c9472f8428ed11000b8b2e

OpenTelemetry integration for the Nx CLI to collect anonymous usage data and performance traces.

**User Consent Flow**:
- First Nx command prompts: "Would you like to help improve Nx with anonymous data?"
- Shows what is/isn't collected, links to privacy policy
- Answer stored in `~/.nxrc` file (`telemetry: true/false`)
- Users who opt out are never asked again
- Repo-level override: `nx.json` can enable telemetry even if user's `.nxrc` has it disabled

**What Gets Collected**:
- Generator usage by plugin (e.g., `@nx/js` used 6 times, `@nx/vue` 3 times)
- Breakdown by specific generator (e.g., `@nx/js:library`)
- OS distribution
- Daemon enabled/disabled usage
- Command execution timings

**Grafana Dashboards** (built by Max):
- Generator usage by plugin (pie chart)
- Generator breakdown (bar chart)
- OS distribution
- Daemon usage stats
- Project graph construction time histogram

**Trace Details**:
- `projectGraphCreate` timing (e.g., 1.18s breakdown by stage)
- `taskGraph` creation time
- Individual task execution time
- Number of times project graph is called per command

**Use Cases**:
- Identify performance issues before they become GitHub issues
- Monitor project graph creation time as workspaces grow
- Detect excessive internal calls (e.g., generator calling `projectGraphCreate` 4 times)
- Data-driven improvements to Nx

*Note: Max is at an AI conference in Japan, so Colum recorded the demo.*

### 3. Leonard - AI QA for Docs & Website
**By**: James
**Demo**: https://www.loom.com/share/7aee66abd91b4251ab3c544c72074faa

Named after the protagonist in Christopher Nolan's *Memento* (who has no long-term memory but can still perform tasks) - reflects how the AI has no persistent memory but can execute complex workflows.

**What It Is**: Automated QA for nx.dev/docs using Claude Code SDK. Three workflows:

#### 1. Content Reviewer (runs nightly)
Orchestrator agent invokes expert sub-agents:
- **Proofreader**: Typos, logical inconsistencies, formatting issues
- **Language Expert**: Technical documentation writing quality

Process:
- Loads page in Playwright browser
- Takes full-size screenshots
- Navigates tabs/accordions to capture all states
- Scrapes content
- Hands screenshots + text to expert sub-agents
- Produces structured output → **creates Linear issues automatically**

Example findings:
- Flags like `--no-distribution` used without explanation
- Prerequisites mentioned inconsistently
- Capitalization inconsistencies ("Self-Healing CI" vs "self-healing CI")

#### 2. New User Persona
Junior-to-mid developer with **zero NX knowledge**. Ignores Claude's training data about NX.
- Starts at docs, tries to get up and running
- Actually runs `create-nx-workspace`, explores the graph, follows README steps
- Produces experience report (rated docs 8.5/10)
- Feedback: Sidebar overwhelming, too many options for beginners

#### 3. NX Skeptic Persona
TurboRepo loyalist with biases: "NX is bloated, over-engineered, locks you in"
- Actually creates a TurboRepo project
- Follows the TurboRepo migration guide
- Runs real comparisons
- Result: "They weren't lying. This is exactly as minimal as they claimed."
- Skeptic verdict: **Recommend switching (with caveats)**
- Called out: NX Cloud promotion feels aggressive, but NX Graph is "gorgeous" and "worth the price of admission alone"

**Output**:
- Linear issues created automatically (to James Docs shadow team for testing)
- Reports committed back to Leonard repo
- Links to GitHub Actions workflow + raw Claude logs for debugging
- Issues include direct links to problematic page sections

**Future Ideas**:
- Hash-based change detection to only re-review modified pages
- Persona explorations weekly, content review multiple times daily
- More personas: Enterprise dev, Hipster dev (only cares about cutting-edge tools)
- Extend to NX Cloud onboarding with real GitHub credentials
- Deeper explorations: add plugins, combine stacks, stress-test edge cases

**Why This Matters**: No dedicated QA role anymore. AI-powered QA catches issues that slip through PR reviews, especially in documentation that hasn't been touched in a while.

### 4. Self-Healing Screen UI Redesign
**By**: Ben
**Demo**: https://www.loom.com/share/0d5379d0ef1244eebfa7dcc157d936eb

**Goal**: Create a UI more familiar to IDE/text editor users - see everything at once (tree view, code diff, chat sidebar) instead of tabbed navigation.

**Current UI Issues**:
- Three separate tabs: Code diff, Failed tasks, AI Analysis
- Constrained by app's max-width (doesn't use full browser width)
- Can't see everything at once

**New UI Changes**:

1. **All-at-once layout**:
   - Left: Tree view + code diff
   - Right: AI analysis panel
   - Top: Failed task count with expandable details
   - Full browser width utilization (resizable panels)

2. **Header improvements**:
   - Shows "nx will push a commit into [branch-name]" for clarity
   - New button to copy direct link to screen (for Slack sharing)
   - Badge, commit message, and date retained

3. **Task details**: Click failed task count to see logs and details inline

4. **New "Apply Fix" dialog**:
   - Confirm/edit commit message
   - Shows "What happens next":
     - CI will automatically rerun
     - You can merge the PR directly
     - Fix is reversible until PR is merged
   - **New feature**: Option to enable auto-apply for this command workspace-wide
     - "I see value in this fix, why not auto-apply similar fixes automatically?"

**Status**: Working experiment - all previous features preserved, nothing hardcoded. More work needed to productionize.

### 5. AI Chat for Nx Graph + Executive Summary
**By**: Jason
**Demo**: https://www.loom.com/share/2fe01cdf8f9546ffa67ca9791c86eed0

Two successful experiments integrating AI into the graph application:

#### Part 1: AI Navigation & Configuration Editing

**Claude button in Graph UI** (top right):
- Navigate the graph with natural language: "Show me the project details for nx"
- Uses Claude SDK with user's own API tokens (no cost to us)
- Ctrl+K opens command palette

**New Project Details Panel**:
- Added Dependencies and Dependents lists
- Click to navigate between projects

**AI-Powered Config Editing** (the "magical" part):
- Tell the AI about missing inputs/outputs in natural language
- Example: "The build-native target is missing cargo.toml and cargo.lock files from workspace root. Can you add them?"
- AI edits `project.json` directly and refreshes the graph
- Needs more work on system prompts, but proof of concept works

**"Explain Project" button**: Ask Claude to analyze project files and explain what it does (ran out of tokens before demo)

#### Part 2: Executive Summary Report

A page designed for senior engineers/CTOs to get high-level visibility:

**Highlights Section** (auto-generated from git commits):
- NX 22 release summary
- New plugins introduced
- Core infrastructure changes
- "I didn't write any of this by hand"

**Activity Heatmap**:
- Files changed per project over last 3 months
- Example: AstroDocs had tons of changes in September, Docs in October
- NX had ~700 files changed across commits
- Maven (newly added) touched 98 files

**Monthly Breakdown**:
- September: Preparing NX 22
- October: Platform expansion, .NET integration
- November: Stability, daemon reliability, plugin performance

**Work in Progress** (from open PRs):
- Summarizes what's coming if current PRs land
- Example: Leo's work on batch tasks, process killing

**Use Cases**:
- Give RBC visibility across their entire monorepo
- Victor could see what's happening in Ocean/NX Cloud
- Prints nicely to PDF for email sharing

**Other attempts** (didn't pan out this time):
- GitHub triage integration to auto-fix ~10% of issues before they reach humans

### 6. Workspace Contributions Dashboard/Analytics
**By**: Mark
**Demo**: https://www.loom.com/share/3ebc80053319404986f5ca540cce0bfb

**Background**: Dix requested this feature last year - ability to track developer contributions to workspaces.

**Current Implementation** (workspace level, similar to other analytics pages):

**Main Dashboard**:
- Runs per contributor
- Pass/fail counts
- Self-healing fixes applied vs rejected
- Failure rates
- Activity over time chart
- Downloadable data (CSV)

**Contributor Detail View** (click on a person):
- Breakdown over time
- Filter by date range
- Filter by specific days or branches

**Demo Notes**:
- Running on Ocean workspace in staging
- Only 2 weeks of data retained (not 30 days)
- Numbers lower due to US holiday week

**Future Ideas**:
- Organization-level view (not just workspace)
- AI prompt to describe desired data format → returns custom CSV
- "Throw the whole dataset at Haiku and let them describe what they want"

**Takeaway**: Simple but useful - built in just a couple hours during hackday.

---

## Overall Impressions

Everyone was happy and had fun. We got some good results despite not having a lot of communication or interactions throughout the day.

## Retrospective

The lack of communication was somewhat expected since we're all remote. If we do this again, we should consider having more coordination and fanfare to make it feel more collaborative.
