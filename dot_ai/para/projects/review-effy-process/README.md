# Review Effy process

The review process has two issues to address:

1. January reviews are too compressed given the January 31st bonus calculation deadline—consider shifting to November/December before the holiday break instead;
2. For the June/July mid-year review round, consider whether we need the full review questions or if a smaller-scope checkpoint would be more appropriate for a quick pulse check.
3. Jason noted that we don't have enough reviews per person sometimes. The more feedback the better, but it's a balance of the time spent vs value we obtain.
4. We need self-evaluations and also ones for leadership, managers.
5. Do managers need to spend more time gather and providing their reviews outside of summarizing Effy?
6. The open-ended question "What's one thign they could do to increase their impact" provides very little value --- it's always "share more Loom", "Speak up more", etc.

---

## Recommended question set

Here's a proposed redesign, with rationale:

**Rating questions (consistent scale: Strongly Agree → Strongly Disagree + Can't Answer):**

1. **"This person consistently demonstrates the skills and judgment expected at their level."** — Keeps your existing Q1, reworded to be more precise about judgment, not just knowledge.

2. **"I would always want this person on my team."** — Keeper test proxy. This single question is your best signal for under/overperformance.

3. **"This person makes the people around them more effective."** — Replaces both your collaboration and "enjoyable to work with" questions. It's outcome-oriented rather than personality-oriented.

**Open-ended questions (all required, each a separate field):**

4. **"What should this person CONTINUE doing?"** — Captures strengths with specificity.

5. **"What should this person STOP doing, or do less of?"** — Forces critical feedback. This is the single most important change you can make.

6. **"What should this person START doing, or do more of?"** — Development-oriented, forward-looking.

7. **"Describe a specific example of impact this person had in the last six months."** — Replaces your "most valuable contribution" with a prompt for evidence, useful for bonus calibration.

---

## Why this is better than what you have

Your current survey has 4 rating questions and 2 open-ended ones. The ratings are doing most of the work, but ratings without examples are hard to act on. The proposed set flips the ratio: 3 ratings (for signal) and 4 open-ended (for actionability). The Stop/Start/Continue structure has been proven at Netflix and is now widely adopted precisely because it solves the "everyone is too nice" problem — Netflix stresses the need to encourage actionable feedback focused on development areas, noting that positive actionable feedback is fine, but keeping it balanced matters most.

The biggest risk is that this takes longer to fill out. You could mitigate this by reducing the number of peers each person reviews (3-4 rather than 5+) and setting a 2-3 sentence minimum rather than expecting paragraphs.

---

**Key resources:**

- [Pragmatic Engineer on performance reviews](https://blog.pragmaticengineer.com/performance-reviews-for-software-engineers/) — best engineering-specific treatment
- [Worklytics on designing reviews for software teams](https://www.worklytics.co/blog/designing-effective-performance-reviews-for-software-development-teams) — good on structured vs open-ended balance
- [INSEAD research on strategic behavior in peer reviews](https://knowledge.insead.edu/leadership-organisations/flawed-feedback-problem-peer-reviews) — important for understanding why niceness is rational behavior, not just cultural
- [Netflix feedback culture breakdown](https://www.performyard.com/articles/netflix-company-culture) — 4A framework and Stop/Start/Continue in practice

Want me to put together a concrete Notion page or doc with the revised questions and instructions for reviewers?
